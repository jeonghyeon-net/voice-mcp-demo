#!/usr/bin/env python3
"""Voice MCP Server - Claude Codeìš© ìŒì„± ì…ì¶œë ¥"""

import warnings
warnings.filterwarnings("ignore")

import re
import time
import numpy as np
import sounddevice as sd
import webrtcvad
import mlx_whisper
import alkana
from kokoro import KPipeline
from mcp.server.fastmcp import FastMCP

# ì•ŒíŒŒë²³ â†’ ì¹´íƒ€ì¹´ë‚˜
ALPHA_TO_KANA = {
    'a': 'ã‚¨ãƒ¼', 'b': 'ãƒ“ãƒ¼', 'c': 'ã‚·ãƒ¼', 'd': 'ãƒ‡ã‚£ãƒ¼', 'e': 'ã‚¤ãƒ¼',
    'f': 'ã‚¨ãƒ•', 'g': 'ã‚¸ãƒ¼', 'h': 'ã‚¨ã‚¤ãƒ', 'i': 'ã‚¢ã‚¤', 'j': 'ã‚¸ã‚§ãƒ¼',
    'k': 'ã‚±ãƒ¼', 'l': 'ã‚¨ãƒ«', 'm': 'ã‚¨ãƒ ', 'n': 'ã‚¨ãƒŒ', 'o': 'ã‚ªãƒ¼',
    'p': 'ãƒ”ãƒ¼', 'q': 'ã‚­ãƒ¥ãƒ¼', 'r': 'ã‚¢ãƒ¼ãƒ«', 's': 'ã‚¨ã‚¹', 't': 'ãƒ†ã‚£ãƒ¼',
    'u': 'ãƒ¦ãƒ¼', 'v': 'ãƒ–ã‚¤', 'w': 'ãƒ€ãƒ–ãƒªãƒ¥ãƒ¼', 'x': 'ã‚¨ãƒƒã‚¯ã‚¹', 'y': 'ãƒ¯ã‚¤', 'z': 'ã‚¼ãƒƒãƒˆ'
}

def preprocess_for_tts(text: str) -> str:
    """ì˜ì–´/ìˆ«ìë¥¼ ì¼ë³¸ì–´ ë°œìŒìœ¼ë¡œ ë³€í™˜"""
    # ìˆ«ì â†’ ì¼ë³¸ì–´
    num_ja = {'0': 'ã‚¼ãƒ­', '1': 'ã„ã¡', '2': 'ã«', '3': 'ã•ã‚“', '4': 'ã‚ˆã‚“',
              '5': 'ã”', '6': 'ã‚ã', '7': 'ãªãª', '8': 'ã¯ã¡', '9': 'ãã‚…ã†'}
    for num, ja in num_ja.items():
        text = text.replace(num, ja)

    # ì˜ì–´ ë‹¨ì–´ â†’ ì¹´íƒ€ì¹´ë‚˜
    def replace_english(match):
        word = match.group(0)
        kana = alkana.get_kana(word.lower())
        if kana:
            return kana
        return ''.join(ALPHA_TO_KANA.get(c.lower(), c) for c in word)

    text = re.sub(r'[A-Za-z]+', replace_english, text)
    return text

mcp = FastMCP("voice")

# ëª¨ë¸ ì‚¬ì „ ë¡œë“œ
_tts = None
_whisper_loaded = False
_first_load_done = False

def get_tts():
    global _tts
    if _tts is None:
        _tts = KPipeline(lang_code='j', repo_id='hexgrad/Kokoro-82M')
    return _tts

def warmup_whisper():
    """Whisper ëª¨ë¸ ì‚¬ì „ ë¡œë“œ"""
    global _whisper_loaded
    if not _whisper_loaded:
        mlx_whisper.transcribe(
            np.zeros(16000, dtype=np.float32),
            path_or_hf_repo="mlx-community/whisper-medium-mlx"
        )
        _whisper_loaded = True

def first_load_notice():
    """ì²« ë¡œë“œ ì‹œ ì•ˆë‚´ ìŒì„±"""
    tts = get_tts()
    for _, _, audio in tts("ã—ã‚‡ãã‹ã¡ã‚…ã†ã€ã—ã°ã‚‰ããŠã¾ã¡ãã ã•ã„", voice="jf_alpha", speed=1.2):
        if audio is not None:
            sd.play(audio, 24000)
            sd.wait()
            break
    warmup_whisper()

SAMPLE_RATE = 16000
FRAME_DURATION_MS = 30
FRAME_SIZE = int(SAMPLE_RATE * FRAME_DURATION_MS / 1000)

# íš¨ê³¼ìŒ ë¯¸ë¦¬ ìƒì„±
def _generate_beep(freq: int, duration: float, volume: float) -> np.ndarray:
    t = np.linspace(0, duration, int(24000 * duration), False)
    tone = np.sin(2 * np.pi * freq * t) * volume
    fade = int(24000 * 0.02)
    tone[:fade] *= np.linspace(0, 1, fade)
    tone[-fade:] *= np.linspace(1, 0, fade)
    return tone.astype(np.float32)

_beep_start_sound = _generate_beep(600, 0.1, 0.4)
_beep_end_sound = _generate_beep(400, 0.08, 0.3)

def beep_start():
    """ë“£ê¸° ì‹œì‘ íš¨ê³¼ìŒ"""
    sd.play(_beep_start_sound, 24000)
    sd.wait()
    time.sleep(0.3)

def beep_end():
    """ë“£ê¸° ì¢…ë£Œ íš¨ê³¼ìŒ"""
    sd.play(_beep_end_sound, 24000)
    sd.wait()

# ëª¨ë¸ì€ ì²« ì‚¬ìš© ì‹œ ë¡œë“œë¨ (lazy loading)


@mcp.tool()
def listen(timeout_seconds: int = 120, language: str = "ko") -> str:
    """
    ë§ˆì´í¬ë¡œ ìŒì„±ì„ ë“£ê³  í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

    ì‚¬ìš©ìê°€ "listen", "ë“£ê¸°", "ìŒì„±" ë“±ì„ ì…ë ¥í•˜ë©´ ì´ ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ì„¸ìš”.

    âš ï¸ ë‹¤ë¥¸ ë„êµ¬ í˜¸ì¶œ ì „í›„ë¡œ speak() í˜¸ì¶œ í•„ìˆ˜. ì§„í–‰ ìƒí™©ë„ ìˆ˜ì‹œë¡œ speak().

    Args:
        timeout_seconds: ìµœëŒ€ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
        language: ì¸ì‹ ì–¸ì–´ (ko, en, ja ë“±)

    Returns:
        ì¸ì‹ëœ í…ìŠ¤íŠ¸
    """
    global _first_load_done
    if not _first_load_done:
        first_load_notice()
        _first_load_done = True

    vad = webrtcvad.Vad(2)
    speech_buffer = []
    silence_count = 0
    is_speaking = False

    SILENCE_THRESHOLD = 20
    MIN_SPEECH_FRAMES = 5
    MAX_SPEECH_FRAMES = 1000  # ì•½ 30ì´ˆ ì œí•œ
    max_frames = int(timeout_seconds * SAMPLE_RATE / FRAME_SIZE)
    frame_count = 0

    beep_start()  # ğŸ”Š ë“£ê¸° ì‹œì‘

    captured_audio = None
    with sd.InputStream(samplerate=SAMPLE_RATE, channels=1, dtype=np.float32, blocksize=FRAME_SIZE) as stream:
        # ì‹œì‘ ì‹œ ë²„í¼ ë¹„ìš°ê¸° (stale ë°ì´í„° ì œê±°)
        for _ in range(10):
            stream.read(FRAME_SIZE)

        while frame_count < max_frames:
            chunk, _ = stream.read(FRAME_SIZE)
            chunk = chunk.flatten()
            chunk_int16 = (chunk * 32767).astype(np.int16)
            frame_count += 1

            try:
                is_speech = vad.is_speech(chunk_int16.tobytes(), SAMPLE_RATE)
            except:
                is_speech = False

            if is_speech:
                if not is_speaking:
                    is_speaking = True
                speech_buffer.append(chunk)
                silence_count = 0

                # ë„ˆë¬´ ê¸¸ë©´ ê°•ì œ ì¢…ë£Œ
                if len(speech_buffer) >= MAX_SPEECH_FRAMES:
                    captured_audio = np.concatenate(speech_buffer)
                    break
            elif is_speaking:
                silence_count += 1
                speech_buffer.append(chunk)

                if silence_count >= SILENCE_THRESHOLD:
                    if len(speech_buffer) >= MIN_SPEECH_FRAMES:
                        audio_data = np.concatenate(speech_buffer)
                        rms = np.sqrt(np.mean(audio_data ** 2))

                        if rms >= 0.01:
                            captured_audio = audio_data
                            break  # ìŠ¤íŠ¸ë¦¼ ë¨¼ì € ë‹«ê¸°

                    speech_buffer = []
                    silence_count = 0
                    is_speaking = False

    # ìŠ¤íŠ¸ë¦¼ ë‹«íŒ í›„ ì²˜ë¦¬
    if captured_audio is not None:
        beep_end()  # ğŸ”Š ì¸ì‹ ì‹œì‘
        result = mlx_whisper.transcribe(
            captured_audio,
            path_or_hf_repo="mlx-community/whisper-medium-mlx",
            language=language
        )
        text = result.get("text", "").strip()

        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del captured_audio
        import gc
        gc.collect()

        if text:
            return f"""[ì‚¬ìš©ì]: {text}

âš ï¸ ë‹¤ë¥¸ ë„êµ¬ í˜¸ì¶œ ì „í›„ë¡œ speak() í˜¸ì¶œ í•„ìˆ˜. ì§„í–‰ ìƒí™©ë„ ìˆ˜ì‹œë¡œ speak()."""

    return "[íƒ€ì„ì•„ì›ƒ] ìŒì„±ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."


@mcp.tool()
def listen_fixed(duration_seconds: int = 3, language: str = "ko") -> str:
    """
    ì§€ì •ëœ ì‹œê°„ ë™ì•ˆ ë…¹ìŒ í›„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

    Args:
        duration_seconds: ë…¹ìŒ ì‹œê°„ (ì´ˆ)
        language: ì¸ì‹ ì–¸ì–´ (ko, en, ja ë“±)

    Returns:
        ì¸ì‹ëœ í…ìŠ¤íŠ¸
    """
    audio = sd.rec(int(duration_seconds * SAMPLE_RATE), samplerate=SAMPLE_RATE, channels=1, dtype=np.float32)
    sd.wait()
    audio = audio.flatten()

    result = mlx_whisper.transcribe(
        audio,
        path_or_hf_repo="mlx-community/whisper-medium-mlx",
        language=language
    )
    return result.get("text", "").strip()


@mcp.tool()
def speak(text: str, voice: str = "jf_alpha", speed: float = 1.0) -> str:
    """
    ì¼ë³¸ì–´ ìŒì„±ìœ¼ë¡œ ë§í•©ë‹ˆë‹¤.

    âš ï¸ ì˜ì–´/ì•ŒíŒŒë²³ ê¸ˆì§€! ì¼ë³¸ì–´(í•œì/íˆë¼ê°€ë‚˜/ì¹´íƒ€ì¹´ë‚˜)ë§Œ ì‚¬ìš©!

    âš ï¸ speak() í›„ ë°˜ë“œì‹œ listen() í˜¸ì¶œ! ì˜ˆì™¸: ì‚¬ìš©ìê°€ "ë", "ë°”ì´ë°”ì´", "ê³ ë§ˆì›Œ" ë“± ì¢…ë£Œ ì‹ í˜¸ë¥¼ ë³´ëƒˆì„ ë•Œë§Œ ìƒëµ

    Args:
        text: ì¼ë³¸ì–´ í…ìŠ¤íŠ¸
        voice: ìŒì„±
        speed: ì†ë„

    Returns:
        "â†’ listen() í˜¸ì¶œí•˜ì„¸ìš”" (ì¢…ë£Œ ì‹ í˜¸ ì—†ìœ¼ë©´)
    """
    tts = get_tts()
    text_processed = preprocess_for_tts(text)
    for _, _, audio in tts(text_processed, voice=voice, speed=speed):
        if audio is not None:
            sd.play(audio, 24000)
            sd.wait()
            break

    return "â†’ listen() í˜¸ì¶œí•˜ì„¸ìš”"


if __name__ == "__main__":
    mcp.run()
