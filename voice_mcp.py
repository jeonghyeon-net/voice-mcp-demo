#!/usr/bin/env python3
"""Voice MCP Server - Claude Codeìš© ìŒì„± ì…ì¶œë ¥"""

import warnings
warnings.filterwarnings("ignore")

import time
import numpy as np
import sounddevice as sd
import torch
import mlx_whisper
from silero_vad import load_silero_vad
from kokoro import KPipeline
from mcp.server.fastmcp import FastMCP

# Silero VAD ë¡œë“œ
torch.set_num_threads(1)
_vad_model = None

def get_vad():
    global _vad_model
    if _vad_model is None:
        _vad_model = load_silero_vad()
    return _vad_model

mcp = FastMCP("voice")

# ëª¨ë¸ ì‚¬ì „ ë¡œë“œ
_tts = None
_whisper_loaded = False
_first_load_done = False

def get_tts():
    global _tts
    if _tts is None:
        _tts = KPipeline(lang_code='j', repo_id='hexgrad/Kokoro-82M')
    return _tts

def warmup_whisper():
    """Whisper ëª¨ë¸ ì‚¬ì „ ë¡œë“œ"""
    global _whisper_loaded
    if not _whisper_loaded:
        mlx_whisper.transcribe(
            np.zeros(16000, dtype=np.float32),
            path_or_hf_repo="mlx-community/whisper-medium-mlx"
        )
        _whisper_loaded = True

def first_load_notice():
    """ì²« ë¡œë“œ ì‹œ ì•ˆë‚´ ìŒì„±"""
    tts = get_tts()
    for _, _, audio in tts("ã—ã‚‡ãã‹ã¡ã‚…ã†ã€ã—ã°ã‚‰ããŠã¾ã¡ãã ã•ã„", voice="jf_alpha", speed=1.2):
        if audio is not None:
            sd.play(audio, 24000)
            sd.wait()
            break
    warmup_whisper()
    # VAD ì›œì—…
    vad = get_vad()
    dummy = torch.zeros(512)
    vad(dummy, SAMPLE_RATE)

SAMPLE_RATE = 16000
FRAME_DURATION_MS = 30
FRAME_SIZE = int(SAMPLE_RATE * FRAME_DURATION_MS / 1000)

# íš¨ê³¼ìŒ ë¯¸ë¦¬ ìƒì„±
def _generate_beep(freq: int, duration: float, volume: float) -> np.ndarray:
    t = np.linspace(0, duration, int(24000 * duration), False)
    tone = np.sin(2 * np.pi * freq * t) * volume
    fade = int(24000 * 0.02)
    tone[:fade] *= np.linspace(0, 1, fade)
    tone[-fade:] *= np.linspace(1, 0, fade)
    return tone.astype(np.float32)

_beep_start_sound = _generate_beep(600, 0.1, 0.4)
_beep_end_sound = _generate_beep(400, 0.08, 0.3)

def beep_start():
    """ë“£ê¸° ì‹œì‘ íš¨ê³¼ìŒ"""
    sd.play(_beep_start_sound, 24000)
    sd.wait()
    time.sleep(0.3)

def beep_end():
    """ë“£ê¸° ì¢…ë£Œ íš¨ê³¼ìŒ"""
    sd.play(_beep_end_sound, 24000)
    sd.wait()

# ëª¨ë¸ì€ ì²« ì‚¬ìš© ì‹œ ë¡œë“œë¨ (lazy loading)


@mcp.tool()
def listen(timeout_seconds: int = 300, language: str = "ko") -> str:
    """
    ë§ˆì´í¬ë¡œ ìŒì„±ì„ ë“£ê³  í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

    ì‚¬ìš©ìê°€ "listen", "ë“£ê¸°", "ìŒì„±" ë“±ì„ ì…ë ¥í•˜ë©´ ì´ ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ì„¸ìš”.

    âš ï¸ ë‹¤ë¥¸ ë„êµ¬ í˜¸ì¶œ ì „í›„ë¡œ speak() í˜¸ì¶œ í•„ìˆ˜. ì§„í–‰ ìƒí™©ë„ ìˆ˜ì‹œë¡œ speak().

    Args:
        timeout_seconds: ìµœëŒ€ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
        language: ì¸ì‹ ì–¸ì–´ (ko, en, ja ë“±)

    Returns:
        ì¸ì‹ëœ í…ìŠ¤íŠ¸
    """
    global _first_load_done
    if not _first_load_done:
        first_load_notice()
        _first_load_done = True

    vad_model = get_vad()

    CHUNK_SIZE = 512  # Silero VAD ê¶Œì¥ í¬ê¸°
    MAX_DURATION = 30  # ìµœëŒ€ ë…¹ìŒ 30ì´ˆ
    SILENCE_DURATION = 1.5  # 1.5ì´ˆ ì¹¨ë¬µ í›„ ì¢…ë£Œ
    MIN_SPEECH_DURATION = 0.5  # ìµœì†Œ 0.5ì´ˆ ë°œí™”í•´ì•¼ ìœ íš¨

    beep_start()  # ğŸ”Š ë“£ê¸° ì‹œì‘

    audio_buffer = []
    is_speaking = False
    silence_samples = 0
    speech_samples = 0  # ì‹¤ì œ ë°œí™” ìƒ˜í”Œ ìˆ˜
    consecutive_speech = 0  # ì—°ì† ìŒì„± í”„ë ˆì„
    start_time = time.time()

    captured_audio = None
    with sd.InputStream(samplerate=SAMPLE_RATE, channels=1, dtype=np.float32, blocksize=CHUNK_SIZE) as stream:
        # ë²„í¼ ë¹„ìš°ê¸°
        for _ in range(5):
            stream.read(CHUNK_SIZE)

        while (time.time() - start_time) < timeout_seconds:
            chunk, _ = stream.read(CHUNK_SIZE)
            chunk = chunk.flatten()

            # Silero VADë¡œ ìŒì„± í™•ë¥  ê³„ì‚°
            try:
                chunk_tensor = torch.from_numpy(chunk).float()
                speech_prob = vad_model(chunk_tensor, SAMPLE_RATE).item()
            except Exception as e:
                speech_prob = 0.0

            # ë³¼ë¥¨ ì²´í¬ (RMS) - ë°°ê²½ ì†ŒìŒ í•„í„°ë§
            rms = np.sqrt(np.mean(chunk ** 2))
            is_voice = speech_prob > 0.85 and rms > 0.02

            if is_voice:
                consecutive_speech += 1
                if not is_speaking and consecutive_speech >= 5:  # 5í”„ë ˆì„ ì—°ì† ìŒì„±ì´ì–´ì•¼ ì‹œì‘
                    is_speaking = True
                if is_speaking:
                    audio_buffer.append(chunk)
                    speech_samples += len(chunk)
                silence_samples = 0

                # ìµœëŒ€ ê¸¸ì´ ì²´í¬
                if len(audio_buffer) * CHUNK_SIZE >= MAX_DURATION * SAMPLE_RATE:
                    captured_audio = np.concatenate(audio_buffer)
                    break
            else:
                consecutive_speech = 0  # ì—°ì† ìŒì„± ë¦¬ì…‹

            if not is_voice and is_speaking:
                audio_buffer.append(chunk)
                silence_samples += len(chunk)

                # ìµœì†Œ ë°œí™” ì‹œê°„ ì¶©ì¡± + ì¹¨ë¬µ ì§€ì† ì‹œì—ë§Œ ì¢…ë£Œ
                if speech_samples >= MIN_SPEECH_DURATION * SAMPLE_RATE:
                    if silence_samples >= SILENCE_DURATION * SAMPLE_RATE:
                        captured_audio = np.concatenate(audio_buffer)
                        break
                else:
                    # ë°œí™”ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ë¦¬ì…‹ (ì¡ìŒìœ¼ë¡œ ê°„ì£¼)
                    if silence_samples >= SILENCE_DURATION * SAMPLE_RATE:
                        audio_buffer = []
                        is_speaking = False
                        speech_samples = 0
                        silence_samples = 0

    # ìŠ¤íŠ¸ë¦¼ ë‹«íŒ í›„ ì²˜ë¦¬
    if captured_audio is not None and len(captured_audio) > SAMPLE_RATE * 0.3:
        beep_end()  # ğŸ”Š ì¸ì‹ ì‹œì‘
        result = mlx_whisper.transcribe(
            captured_audio,
            path_or_hf_repo="mlx-community/whisper-medium-mlx",
            language=language
        )
        text = result.get("text", "").strip()

        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del captured_audio
        import gc
        gc.collect()

        if text:
            return f"""[ì‚¬ìš©ì]: {text}

âš ï¸ ë‹¤ë¥¸ ë„êµ¬ í˜¸ì¶œ ì „í›„ë¡œ speak() í˜¸ì¶œ í•„ìˆ˜. ì§„í–‰ ìƒí™©ë„ ìˆ˜ì‹œë¡œ speak()."""

    return "[íƒ€ì„ì•„ì›ƒ] ìŒì„±ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."


@mcp.tool()
def listen_fixed(duration_seconds: int = 3, language: str = "ko") -> str:
    """
    ì§€ì •ëœ ì‹œê°„ ë™ì•ˆ ë…¹ìŒ í›„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

    Args:
        duration_seconds: ë…¹ìŒ ì‹œê°„ (ì´ˆ)
        language: ì¸ì‹ ì–¸ì–´ (ko, en, ja ë“±)

    Returns:
        ì¸ì‹ëœ í…ìŠ¤íŠ¸
    """
    audio = sd.rec(int(duration_seconds * SAMPLE_RATE), samplerate=SAMPLE_RATE, channels=1, dtype=np.float32)
    sd.wait()
    audio = audio.flatten()

    result = mlx_whisper.transcribe(
        audio,
        path_or_hf_repo="mlx-community/whisper-medium-mlx",
        language=language
    )
    return result.get("text", "").strip()


@mcp.tool()
def speak(text: str, voice: str = "jf_alpha", speed: float = 1.0) -> str:
    """
    ì¼ë³¸ì–´ ìŒì„±ìœ¼ë¡œ ë§í•©ë‹ˆë‹¤.

    âš ï¸ ì˜ì–´/ì•ŒíŒŒë²³ ê¸ˆì§€! ì¼ë³¸ì–´(í•œì/íˆë¼ê°€ë‚˜/ì¹´íƒ€ì¹´ë‚˜)ë§Œ ì‚¬ìš©!

    âš ï¸ speak() í›„ ë°˜ë“œì‹œ listen() í˜¸ì¶œ! ì˜ˆì™¸: ì‚¬ìš©ìê°€ "ë", "ë°”ì´ë°”ì´", "ê³ ë§ˆì›Œ" ë“± ì¢…ë£Œ ì‹ í˜¸ë¥¼ ë³´ëƒˆì„ ë•Œë§Œ ìƒëµ

    Args:
        text: ì¼ë³¸ì–´ í…ìŠ¤íŠ¸
        voice: ìŒì„±
        speed: ì†ë„

    Returns:
        "â†’ listen() í˜¸ì¶œí•˜ì„¸ìš”" (ì¢…ë£Œ ì‹ í˜¸ ì—†ìœ¼ë©´)
    """
    tts = get_tts()
    for _, _, audio in tts(text, voice=voice, speed=speed):
        if audio is not None:
            sd.play(audio, 24000)
            sd.wait()
            break

    return "â†’ listen() í˜¸ì¶œí•˜ì„¸ìš”"


if __name__ == "__main__":
    mcp.run()
